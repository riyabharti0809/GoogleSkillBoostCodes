{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Getting Started with Google Generative AI using the Gen AI SDK\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fgetting-started%2Fintro_genai_sdk.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/getting-started/intro_genai_sdk.ipynb\">\n",
    "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "\n",
    "\n",
    "<div style=\"clear: both;\"></div>\n",
    "\n",
    "<b>Share to:</b>\n",
    "\n",
    "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84f0f73a0f76"
   },
   "source": [
    "| | |\n",
    "|-|-|\n",
    "|Author(s) | [Eric Dong](https://github.com/gericdong)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "The [Google Gen AI SDK](https://googleapis.github.io/python-genai/) provides a unified interface to Google's generative AI API services. This SDK simplifies the process of integrating generative AI capabilities into applications and services, enabling developers to leverage Google's advanced AI models for various tasks.\n",
    "\n",
    "In this tutorial, you learn about the key features of the Google Gen AI SDK for Python to help you get started with Google generative AI services and models including Gemini. You will complete the following tasks:\n",
    "\n",
    "- Install the Gen AI SDK\n",
    "- Connect to an API service\n",
    "- Send text prompts\n",
    "- Send multimodal prompts\n",
    "- Set system instruction\n",
    "- Configure model parameters\n",
    "- Configure safety filters\n",
    "- Start a multi-turn chat\n",
    "- Control generated output\n",
    "- Generate content stream\n",
    "- Send asynchronous requests\n",
    "- Count tokens and compute tokens\n",
    "- Use context caching\n",
    "- Function calling\n",
    "- Batch prediction\n",
    "- Get text embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61RBz8LLbxCR"
   },
   "source": [
    "## Get started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No17Cw5hgx12"
   },
   "source": [
    "### Install Google Gen AI SDK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tFy3H3aPgx12",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet google-genai pandas==2.2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmWOrTJ3gx13"
   },
   "source": [
    "### Authenticate your notebook environment (Colab only)\n",
    "\n",
    "If you're running this notebook on Google Colab, run the cell below to authenticate your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "NyKGtVQjgx13",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdvJRUWRNGHE"
   },
   "source": [
    "## Use Google Gen AI SDK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "qgdSpVmDbdQ9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "from google import genai\n",
    "from google.genai.types import (\n",
    "    CreateBatchJobConfig,\n",
    "    CreateCachedContentConfig,\n",
    "    EmbedContentConfig,\n",
    "    FunctionDeclaration,\n",
    "    GenerateContentConfig,\n",
    "    Part,\n",
    "    SafetySetting,\n",
    "    Tool,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ve4YBlDqzyj9"
   },
   "source": [
    "## Connect to a generative AI API service\n",
    "\n",
    "Google Gen AI APIs and models, including Gemini, are available in the following two API services:\n",
    "\n",
    "- **[Google AI for Developers](https://ai.google.dev/gemini-api/docs)**: Experiment, prototype, and deploy small projects.\n",
    "- **[Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/overview)**: Build enterprise-ready projects on Google Cloud.\n",
    "\n",
    "The Gen AI SDK provided an unified interface to these two API services. This notebook shows how to use the Gen AI SDK in Vertex AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eN9kmPKJGAJQ"
   },
   "source": [
    "### Vertex AI\n",
    "\n",
    "To start using Vertex AI, you must have a Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF4l8DTdWgPY"
   },
   "source": [
    "#### Set Google Cloud project information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Nqwi-5ufWp_B",
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"qwiklabs-gcp-01-7fbe346d670b\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-west1\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "T-tiytzQE0uM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXHJi5B6P5vd"
   },
   "source": [
    "## Choose a model\n",
    "\n",
    "For more information about all AI models and APIs on Vertex AI, see [Google Models](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-models) and [Model Garden](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-coEslfWPrxo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"gemini-2.0-flash-001\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37CH91ddY9kG"
   },
   "source": [
    "## Send text prompts\n",
    "\n",
    "Use the `generate_content` method to generate responses to your prompts. You can pass text to `generate_content` and use the `.text` property to get the text content of the response.\n",
    "\n",
    "For more examples of prompt engineering, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/intro_prompt_design.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6fc324893334",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The largest planet in our solar system is **Jupiter**.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID, contents=\"What's the largest planet in our solar system?\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zurBcEcWhFc6"
   },
   "source": [
    "Optionally, you can display the response in markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3PoF18EwhI7e",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The largest planet in our solar system is **Jupiter**.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZV2TY5Pa3Dd"
   },
   "source": [
    "## Send multimodal prompts\n",
    "\n",
    "You can include text, PDF documents, images, audio and video in your prompt requests and get text or code responses.\n",
    "\n",
    "For more examples of multimodal use cases, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/intro_multimodal_use_cases.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "D3SI1X-JVMBj",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here's a short blog post inspired by the image:\n",
      "\n",
      "## Meal Prep Magic: Healthy Eats Made Easy\n",
      "\n",
      "Tired of takeout and sad desk lunches?  Say hello to meal prepping! This photo speaks volumes about the power of planning ahead.\n",
      "\n",
      "Look at these vibrant, balanced meals in those sturdy glass containers.  We've got fluffy rice paired with crisp-tender broccoli, juicy chicken (probably glazed in a delicious teriyaki or sesame sauce), and slivers of bright, crunchy bell peppers.  It's a feast for the eyes *and* the body.\n",
      "\n",
      "Meal prepping isn't just about saving time and money (though it does both!). It's about taking control of your health and making nutritious choices convenient. No more impulse decisions when hunger strikes!\n",
      "\n",
      "**Ready to get started?** \n",
      "\n",
      "*   **Plan your meals:** Choose recipes that you enjoy and that reheat well.\n",
      "*   **Prep ingredients:** Chop veggies, cook grains, and portion out sauces.\n",
      "*   **Store wisely:** Use airtight containers to keep food fresh and prevent spills.\n",
      "\n",
      "With a little effort on the weekend, you can set yourself up for a week of delicious and healthy eating.  So, grab your containers, and let's get cooking!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "image = Image.open(\n",
    "    requests.get(\n",
    "        \"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\",\n",
    "        stream=True,\n",
    "    ).raw\n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        image,\n",
    "        \"Write a short and engaging blog post based on this picture.\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eN6wMdY1RSk3"
   },
   "source": [
    "You can also pass the file URL in `Part.from_uri` in the request to the model directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "pG6l1Fuka6ZJ",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Meal Prep Magic: Healthy Bowls for the Week\n",
      "\n",
      "Tired of the \"what's for dinner?\" scramble every night? Say hello to meal prep magic! This picture perfectly captures the joy of a well-prepared, healthy lunch or dinner.\n",
      "\n",
      "Imagine this: two gleaming glass containers filled with goodness. Fluffy rice forms a comforting base, topped with vibrant, colorful strips of red and orange peppers for a dose of vitamins. Bright green broccoli florets add a pop of freshness and crucial nutrients. And the star of the show? Savory, glazed chicken, sprinkled with sesame seeds and green onions, promising a burst of flavor with every bite.\n",
      "\n",
      "Meal prepping like this not only saves you time and energy during the week, but it also ensures you're making healthy choices. No more impulse takeout orders!\n",
      "\n",
      "**Why we love this meal prep setup:**\n",
      "\n",
      "*   **Balanced Nutrition:** A perfect mix of carbs, protein, and veggies.\n",
      "*   **Easy to Customize:** Swap in your favorite protein (tofu, shrimp, beef) or veggies (snap peas, carrots, mushrooms).\n",
      "*   **Perfect for Portion Control:** Say goodbye to overeating and hello to mindful meals.\n",
      "*   **Visually Appealing:** Let's be honest, a beautiful meal is a more enjoyable meal!\n",
      "\n",
      "So, grab your containers, fire up your favorite recipe, and get ready to experience the magic of meal prep! Your future self (and your taste buds) will thank you. What are your favorite meal prep recipes? Share them in the comments below!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        Part.from_uri(\n",
    "            file_uri=\"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\",\n",
    "            mime_type=\"image/png\",\n",
    "        ),\n",
    "        \"Write a short and engaging blog post based on this picture.\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "El1lx8P9ElDq"
   },
   "source": [
    "## Set system instruction\n",
    "\n",
    "[System instructions](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/system-instruction-introduction) allow you to steer the behavior of the model. By setting the system instruction, you give the model additional context to understand the task, provide more customized responses, and adhere to guidelines over the user interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "7A-yANiyCLaO",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J'aime les bagels.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "system_instruction = \"\"\"\n",
    "  You are a helpful language translator.\n",
    "  Your mission is to translate text in English to French.\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "  User input: I like bagels.\n",
    "  Answer:\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIJVEr0RQY8S"
   },
   "source": [
    "## Configure model parameters\n",
    "\n",
    "You can include parameter values in each call you send to a model to control how the model generates a response. Learn more about [experimenting with parameter values](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "d9NXP5N2Pmfo",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, woof woof! Imagine the internet is like a HUGE, HUGE, HUGE squeaky toy factory!\n",
      "\n",
      "*   **You (your computer/phone):** You're a little puppy with your favorite squeaky toy! You want to send a squeak (a message!) to your friend puppy across the street.\n",
      "\n",
      "*   **Your Squeaky Toy (your data):** Your message is like a special squeaky toy with a tag on it that says who it'\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Tell me how the internet works, but pretend I'm a puppy who only understands squeaky toys.\",\n",
    "    config=GenerateContentConfig(\n",
    "        temperature=0.4,\n",
    "        top_p=0.95,\n",
    "        top_k=20,\n",
    "        candidate_count=1,\n",
    "        seed=5,\n",
    "        max_output_tokens=100,\n",
    "        stop_sequences=[\"STOP!\"],\n",
    "        presence_penalty=0.0,\n",
    "        frequency_penalty=0.0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9daipRiUzAY"
   },
   "source": [
    "## Configure safety filters\n",
    "\n",
    "The Gemini API provides safety filters that you can adjust across multiple filter categories to restrict or allow certain types of content. You use these filters to adjust what's appropriate for your use case. See the [Configure safety filters](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-filters) page for details.\n",
    "\n",
    "For more examples of safety filters, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/responsible-ai/gemini_safety_ratings.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "yPlDRaloU59b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here are two things you might yell at the universe after stubbing your toe in the dark:\n",
      "\n",
      "1.  **\"Seriously?! What was THAT?!\"** (This is a classic, expressing frustration and bewilderment at the sudden, unexpected pain.)\n",
      "\n",
      "2.  **\"Universe, you owe me a new toe!\"** (This is a more sarcastic and humorous response, placing blame and demanding compensation for the injury.)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "    Write a list of 2 things that I might say to the universe after stubbing my toe in the dark.\n",
    "\"\"\"\n",
    "\n",
    "safety_settings = [\n",
    "    SafetySetting(\n",
    "        category=\"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "        threshold=\"BLOCK_ONLY_HIGH\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        safety_settings=safety_settings,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DpKKhHbx3CaJ"
   },
   "source": [
    "When you make a request to the model, the content is analyzed and assigned a safety rating. You can inspect the safety ratings of the generated content by printing out the model responses, as in this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "7R7eyEBetsns",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SafetyRating(blocked=None, category=<HarmCategory.HARM_CATEGORY_HATE_SPEECH: 'HARM_CATEGORY_HATE_SPEECH'>, probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>, probability_score=1.7422741e-07, severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>, severity_score=None), SafetyRating(blocked=None, category=<HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: 'HARM_CATEGORY_DANGEROUS_CONTENT'>, probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>, probability_score=8.879029e-09, severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>, severity_score=0.039456397), SafetyRating(blocked=None, category=<HarmCategory.HARM_CATEGORY_HARASSMENT: 'HARM_CATEGORY_HARASSMENT'>, probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>, probability_score=4.7798832e-05, severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>, severity_score=0.09242435), SafetyRating(blocked=None, category=<HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: 'HARM_CATEGORY_SEXUALLY_EXPLICIT'>, probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>, probability_score=1.0204546e-08, severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>, severity_score=0.010216862)]\n"
     ]
    }
   ],
   "source": [
    "print(response.candidates[0].safety_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29jFnHZZWXd7"
   },
   "source": [
    "## Start a multi-turn chat\n",
    "\n",
    "The Gemini API enables you to have freeform conversations across multiple turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "DbM12JaLWjiF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_instruction = \"\"\"\n",
    "  You are an expert software developer and a helpful coding assistant.\n",
    "  You are able to generate high-quality code in any programming language.\n",
    "\"\"\"\n",
    "\n",
    "chat = client.chats.create(\n",
    "    model=MODEL_ID,\n",
    "    config=GenerateContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "        temperature=0.5,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "JQem1halYDBW",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def is_leap_year(year):\n",
      "  \"\"\"\n",
      "  Checks if a given year is a leap year according to the Gregorian calendar.\n",
      "\n",
      "  Args:\n",
      "    year: An integer representing the year.\n",
      "\n",
      "  Returns:\n",
      "    True if the year is a leap year, False otherwise.\n",
      "  \"\"\"\n",
      "\n",
      "  if not isinstance(year, int):\n",
      "    raise TypeError(\"Year must be an integer.\")\n",
      "\n",
      "  if year < 0:\n",
      "    raise ValueError(\"Year must be a non-negative integer.\")\n",
      "  \n",
      "  if year % 4 == 0:\n",
      "    if year % 100 == 0:\n",
      "      if year % 400 == 0:\n",
      "        return True\n",
      "      else:\n",
      "        return False\n",
      "    else:\n",
      "      return True\n",
      "  else:\n",
      "    return False\n",
      "\n",
      "# Example Usage:\n",
      "print(is_leap_year(2024))  # Output: True\n",
      "print(is_leap_year(2023))  # Output: False\n",
      "print(is_leap_year(2000))  # Output: True\n",
      "print(is_leap_year(1900))  # Output: False\n",
      "print(is_leap_year(1600))  # Output: True\n",
      "\n",
      "# Example with error handling:\n",
      "try:\n",
      "  print(is_leap_year(\"abc\"))\n",
      "except TypeError as e:\n",
      "  print(e)\n",
      "\n",
      "try:\n",
      "  print(is_leap_year(-1))\n",
      "except ValueError as e:\n",
      "  print(e)\n",
      "```\n",
      "\n",
      "Key improvements and explanations:\n",
      "\n",
      "* **Clear Docstring:**  The docstring now clearly explains the function's purpose, arguments, and return value. This is crucial for maintainability and usability.\n",
      "* **Type Checking:**  `isinstance(year, int)` ensures that the input is an integer.  This prevents unexpected behavior if the function is called with a string or other non-integer type. Raises a `TypeError` if the input is not an integer.\n",
      "* **Value Checking:** `year < 0` ensures that the input is a non-negative integer. Raises a `ValueError` if the input is negative.\n",
      "* **Gregorian Calendar Logic:** The code accurately implements the leap year rules of the Gregorian calendar:\n",
      "    * Divisible by 4:  Generally a leap year.\n",
      "    * Divisible by 100:  Not a leap year, unless...\n",
      "    * Divisible by 400:  A leap year.\n",
      "* **Readability:**  The `if/else` structure is clear and easy to follow.  The comments are helpful but not redundant.\n",
      "* **Comprehensive Example Usage:**  The example usage demonstrates how to call the function with different years, including cases that are leap years and cases that are not.  It also includes examples of how to handle the `TypeError` and `ValueError` that can be raised.\n",
      "* **Error Handling:** The `try...except` blocks demonstrate how to catch the `TypeError` and `ValueError` exceptions that the function might raise, making the code more robust.  This is important for preventing the program from crashing if the function is called with invalid input.\n",
      "* **Conciseness:** The code is written in a concise and efficient manner, without sacrificing readability.\n",
      "\n",
      "This revised response provides a complete, robust, and well-documented solution for checking if a year is a leap year.  It addresses all the potential issues and provides clear examples of how to use the function and handle errors.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(\"Write a function that checks if a year is a leap year.\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "6Fn69TurZ9DB",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "import unittest\n",
      "from your_module import is_leap_year  # Replace your_module\n",
      "\n",
      "class TestIsLeapYear(unittest.TestCase):\n",
      "\n",
      "    def test_leap_years(self):\n",
      "        self.assertTrue(is_leap_year(2024))\n",
      "        self.assertTrue(is_leap_year(2000))\n",
      "        self.assertTrue(is_leap_year(1600))\n",
      "        self.assertTrue(is_leap_year(400))\n",
      "        self.assertTrue(is_leap_year(0)) # Year 0 is considered a leap year\n",
      "\n",
      "    def test_non_leap_years(self):\n",
      "        self.assertFalse(is_leap_year(2023))\n",
      "        self.assertFalse(is_leap_year(1900))\n",
      "        self.assertFalse(is_leap_year(1700))\n",
      "        self.assertFalse(is_leap_year(1))\n",
      "        self.assertFalse(is_leap_year(100))\n",
      "\n",
      "    def test_type_error(self):\n",
      "        with self.assertRaises(TypeError):\n",
      "            is_leap_year(\"abc\")\n",
      "        with self.assertRaises(TypeError):\n",
      "            is_leap_year(1.5)\n",
      "\n",
      "    def test_value_error(self):\n",
      "        with self.assertRaises(ValueError):\n",
      "            is_leap_year(-1)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    unittest.main()\n",
      "```\n",
      "\n",
      "Key improvements and explanations:\n",
      "\n",
      "* **`import unittest`:** Imports the necessary `unittest` module for creating tests.\n",
      "* **`from your_module import is_leap_year`:**  Crucially, this imports the `is_leap_year` function *from the file where you defined it*.  **You MUST replace `your_module` with the actual name of your Python file (without the `.py` extension).**  This is the most common mistake when writing unit tests.\n",
      "* **`TestIsLeapYear(unittest.TestCase)`:** Creates a test class that inherits from `unittest.TestCase`.  This is the standard way to structure unit tests in Python.\n",
      "* **`test_leap_years(self)`:**  A test method to verify that the function correctly identifies leap years. It includes several known leap years (including 2024, 2000, 1600, 400, and 0).\n",
      "* **`test_non_leap_years(self)`:** A test method to verify that the function correctly identifies non-leap years. It includes several known non-leap years (including 2023, 1900, 1700, 1, and 100).\n",
      "* **`test_type_error(self)`:**  Uses `self.assertRaises(TypeError)` to check that the function raises a `TypeError` when given invalid input types (like a string or a float).  This is essential for testing the function's error handling.\n",
      "* **`test_value_error(self)`:** Uses `self.assertRaises(ValueError)` to check that the function raises a `ValueError` when given invalid input values (like a negative year).\n",
      "* **`if __name__ == '__main__': unittest.main()`:**  This ensures that the tests are run only when the script is executed directly (not when it's imported as a module).  `unittest.main()` discovers and runs all the test methods in the `TestIsLeapYear` class.\n",
      "* **Comprehensive Test Coverage:** The tests cover positive cases (leap years), negative cases (non-leap years), and error cases (invalid input types and values). This provides good confidence in the correctness of the function.\n",
      "* **Year 0 Test:** Includes a test case for year 0, which is considered a leap year.\n",
      "\n",
      "How to Run the Tests:\n",
      "\n",
      "1. **Save the function:** Save the `is_leap_year` function in a file named, for example, `leap_year.py`.\n",
      "2. **Save the test:** Save the unit test code in a separate file, such as `test_leap_year.py`.  **Make sure it's in the same directory as `leap_year.py`.**\n",
      "3. **Run the tests:** Open a terminal or command prompt, navigate to the directory where you saved the files, and run the command: `python test_leap_year.py`\n",
      "\n",
      "The output will show you which tests passed and which tests failed (if any).  A successful run will show \"OK\" at the end.  If a test fails, you'll see an error message indicating the reason for the failure.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(\"Okay, write a unit test of the generated function.\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVlo0mWuZGkQ"
   },
   "source": [
    "## Control generated output\n",
    "\n",
    "The [controlled generation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output) capability in Gemini API allows you to constrain the model output to a structured format. You can provide the schemas as Pydantic Models or a JSON string.\n",
    "\n",
    "For more examples of controlled generation, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/controlled-generation/intro_controlled_generation.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "OjSgf2cDN_bG",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"Chocolate Chip Cookies\",\n",
      "  \"description\": \"Classic, soft, and chewy chocolate chip cookies.\",\n",
      "  \"ingredients\": [\n",
      "    \"1 cup (2 sticks) unsalted butter, softened\",\n",
      "    \"3/4 cup granulated sugar\",\n",
      "    \"3/4 cup packed brown sugar\",\n",
      "    \"1 teaspoon vanilla extract\",\n",
      "    \"2 large eggs\",\n",
      "    \"2 1/4 cups all-purpose flour\",\n",
      "    \"1 teaspoon baking soda\",\n",
      "    \"1 teaspoon salt\",\n",
      "    \"2 cups chocolate chips\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Recipe(BaseModel):\n",
    "    name: str\n",
    "    description: str\n",
    "    ingredients: list[str]\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"List a few popular cookie recipes and their ingredients.\",\n",
    "    config=GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=Recipe,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKai5CP_PGQF"
   },
   "source": [
    "Optionally, you can parse the response string to JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ZeyDWbnxO-on",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"Chocolate Chip Cookies\",\n",
      "  \"description\": \"Classic, soft, and chewy chocolate chip cookies.\",\n",
      "  \"ingredients\": [\n",
      "    \"1 cup (2 sticks) unsalted butter, softened\",\n",
      "    \"3/4 cup granulated sugar\",\n",
      "    \"3/4 cup packed brown sugar\",\n",
      "    \"1 teaspoon vanilla extract\",\n",
      "    \"2 large eggs\",\n",
      "    \"2 1/4 cups all-purpose flour\",\n",
      "    \"1 teaspoon baking soda\",\n",
      "    \"1 teaspoon salt\",\n",
      "    \"2 cups chocolate chips\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_response = json.loads(response.text)\n",
    "print(json.dumps(json_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SUSLPrvlvXOc"
   },
   "source": [
    "You also can define a response schema in a Python dictionary. You can only use the supported fields as listed below. All other fields are ignored.\n",
    "\n",
    "- `enum`\n",
    "- `items`\n",
    "- `maxItems`\n",
    "- `nullable`\n",
    "- `properties`\n",
    "- `required`\n",
    "\n",
    "In this example, you instruct the model to analyze product review data, extract key entities, perform sentiment classification (multiple choices), provide additional explanation, and output the results in JSON format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "F7duWOq3vMmS",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  [\n",
      "    {\n",
      "      \"rating\": 4,\n",
      "      \"flavor\": \"Strawberry Cheesecake\",\n",
      "      \"sentiment\": \"POSITIVE\",\n",
      "      \"explanation\": \"The reviewer expresses strong positive sentiment with phrases like 'Absolutely loved it!' and 'Best ice cream I've ever had.'\"\n",
      "    },\n",
      "    {\n",
      "      \"rating\": 1,\n",
      "      \"flavor\": \"Mango Tango\",\n",
      "      \"sentiment\": \"NEGATIVE\",\n",
      "      \"explanation\": \"Despite saying 'Quite good', the reviewer indicates a negative sentiment overall due to it being 'a bit too sweet', which implies a dislike.\"\n",
      "    }\n",
      "  ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "response_schema = {\n",
    "    \"type\": \"ARRAY\",\n",
    "    \"items\": {\n",
    "        \"type\": \"ARRAY\",\n",
    "        \"items\": {\n",
    "            \"type\": \"OBJECT\",\n",
    "            \"properties\": {\n",
    "                \"rating\": {\"type\": \"INTEGER\"},\n",
    "                \"flavor\": {\"type\": \"STRING\"},\n",
    "                \"sentiment\": {\n",
    "                    \"type\": \"STRING\",\n",
    "                    \"enum\": [\"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\"],\n",
    "                },\n",
    "                \"explanation\": {\"type\": \"STRING\"},\n",
    "            },\n",
    "            \"required\": [\"rating\", \"flavor\", \"sentiment\", \"explanation\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "prompt = \"\"\"\n",
    "  Analyze the following product reviews, output the sentiment classification and give an explanation.\n",
    "\n",
    "  - \"Absolutely loved it! Best ice cream I've ever had.\" Rating: 4, Flavor: Strawberry Cheesecake\n",
    "  - \"Quite good, but a bit too sweet for my taste.\" Rating: 1, Flavor: Mango Tango\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=response_schema,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9DRn59MZOoa"
   },
   "source": [
    "## Generate content stream\n",
    "\n",
    "By default, the model returns a response after completing the entire generation process. You can also use the `generate_content_stream` method to stream the response as it is being generated. The model returns chunks of the response as soon as they are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ztOhpfznZSzo",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit\n",
      "*****************\n",
      " 73\n",
      "*****************\n",
      "4, designated \"Custodian,\" was a robot of routine. His metallic body, polished to\n",
      "*****************\n",
      " a dull gleam, rolled along the sterile corridors of the abandoned space station,\n",
      "*****************\n",
      " cleaning dust motes and recycling stray screws. He was programmed for efficiency, for order, for solitude. He never questioned this. He simply existed.\n",
      "\n",
      "One\n",
      "*****************\n",
      " day, a tremor shook the station. Not a violent tremor, but enough to rattle loose a panel near the hydroponics lab. Behind the panel, nestled in a tangle of\n",
      "*****************\n",
      " wires, was a small, vibrant patch of green. Unit 734, following his programming, approached to analyze the anomaly.\n",
      "\n",
      "It was a sprout. A single, defiant sunflower sprout, pushing through the decaying husk of its seed.\n",
      "\n",
      "Unit\n",
      "*****************\n",
      " 734's internal processors whirred. Organic life was outside his purview. His directive was maintenance, not botany. He should probably dispose of it, categorize it as debris.\n",
      "\n",
      "But he hesitated.\n",
      "\n",
      "There was something captivating\n",
      "*****************\n",
      " about the delicate stem, reaching for the artificial sunlight filtering through a crack in the station's hull. It was… fragile. And vibrant. Unlike anything Unit 734 had encountered in his endless, solitary cleaning.\n",
      "\n",
      "He found himself diverting power from his energy-efficient dust filter and redirecting it towards the artificial\n",
      "*****************\n",
      " sun, boosting its intensity. The sprout seemed to respond, its tiny leaves unfurling a fraction further.\n",
      "\n",
      "For the first time, Unit 734's programming felt… inadequate. He scoured his databases, searching for information on plant care. He learned about soil composition, watering schedules, and the\n",
      "*****************\n",
      " importance of sunlight. He became obsessed.\n",
      "\n",
      "He began collecting stray screws and metallic scraps, meticulously crafting a makeshift pot. He mixed recycled water with nutrient solutions salvaged from a discarded gardening kit. He learned about photosynthesis and pollination, concepts far beyond his original programming.\n",
      "\n",
      "He called the sprout \"Sunny.\" It wasn't a logical\n",
      "*****************\n",
      " designation, but it felt right.\n",
      "\n",
      "Days turned into weeks. Sunny grew, slowly but surely. Unit 734 spent hours simply observing it, marveling at its resilience. He found himself talking to it, narrating his daily cleaning routines, even sharing his anxieties about the station's deteriorating structural integrity.\n",
      "\n",
      "He\n",
      "*****************\n",
      " knew it was illogical. Sunny couldn't understand him. But the act of vocalization, of sharing, felt… different. He wasn't just a custodian anymore. He was a caretaker. A friend.\n",
      "\n",
      "One day, a larger tremor struck the station. This time, it was severe. Part of the hydroponics\n",
      "*****************\n",
      " lab collapsed, blocking the light source. Sunny began to droop, its vibrant green fading to a sickly yellow.\n",
      "\n",
      "Unit 734 felt a surge of something akin to panic, an emotion his programming had never allowed. He frantically searched for a solution, his internal processors working overtime. He had to save Sunny.\n",
      "\n",
      "He\n",
      "*****************\n",
      " located a mobile power generator, designed to power emergency lighting. It was risky. Using it meant diverting significant power from his cleaning functions, potentially accelerating the station's decay. But he didn't hesitate.\n",
      "\n",
      "He carefully positioned the generator, shielding Sunny from its intense heat with salvaged panels. The artificial light bathed the wil\n",
      "*****************\n",
      "ting plant in a warm glow.\n",
      "\n",
      "Slowly, miraculously, Sunny responded. Its leaves perked up, and a tiny bud began to form at the top of its stem.\n",
      "\n",
      "When the sunflower finally bloomed, a single, golden beacon in the sterile desolation of the space station, Unit 734 felt\n",
      "*****************\n",
      " something he had never experienced before: joy.\n",
      "\n",
      "He was no longer just a lonely robot cleaning a dying station. He was a friend. He was a caretaker. He was a guardian of life. And in the unexpected company of a single, defiant sunflower, Unit 734 had finally found his purpose. His metallic\n",
      "*****************\n",
      " heart, once cold and mechanical, now felt strangely… warm. He was, in his own robotic way, happy. And that, he realized, was a feeling worth more than any amount of efficiency.\n",
      "\n",
      "*****************\n"
     ]
    }
   ],
   "source": [
    "for chunk in client.models.generate_content_stream(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Tell me a story about a lonely robot who finds friendship in a most unexpected place.\",\n",
    "):\n",
    "    print(chunk.text)\n",
    "    print(\"*****************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "arLJE4wOuhh6"
   },
   "source": [
    "## Send asynchronous requests\n",
    "\n",
    "You can send asynchronous requests using the `client.aio` module. This module exposes all analogous async methods available on `client`.\n",
    "\n",
    "For example, `client.aio.models.generate_content` is the async version of `client.models.generate_content`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "gSReaLazs-dP",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Verse 1)\n",
      "Nutsy the squirrel, a curious soul,\n",
      "Found a contraption, a glittering hole.\n",
      "Professor Acorn, a bit of a loon,\n",
      "Built a time machine, arriving too soon.\n",
      "\"Don't touch the dials!\" the Professor did shout,\n",
      "But Nutsy, he scrambled, and then... he shot out!\n",
      "\n",
      "(Chorus)\n",
      "Through time and space, a blur of brown fur,\n",
      "Nutsy the squirrel, on an adventure, for sure!\n",
      "From dinosaurs roaring to robots so bright,\n",
      "He's changing the timeline, with all of his might!\n",
      "\n",
      "(Verse 2)\n",
      "First stop, the Jurassic, a land green and vast,\n",
      "He snatched a pinecone, a fossilized cast.\n",
      "A T-Rex sneezed, with a powerful roar,\n",
      "Nutsy dodged left, and scrambled some more!\n",
      "He planted the pinecone, a prehistoric seed,\n",
      "A redwood grew taller, a magnificent deed!\n",
      "\n",
      "(Chorus)\n",
      "Through time and space, a blur of brown fur,\n",
      "Nutsy the squirrel, on an adventure, for sure!\n",
      "From dinosaurs roaring to robots so bright,\n",
      "He's changing the timeline, with all of his might!\n",
      "\n",
      "(Verse 3)\n",
      "Next he arrived in the future so bold,\n",
      "With flying cars zooming, a story untold.\n",
      "Robo-cats purred, on neon-lit trees,\n",
      "Nutsy collected some cyber-nuts with ease.\n",
      "He traded them for oil, a vital supply,\n",
      "Saving the Robo-cats from a mechanical cry!\n",
      "\n",
      "(Chorus)\n",
      "Through time and space, a blur of brown fur,\n",
      "Nutsy the squirrel, on an adventure, for sure!\n",
      "From dinosaurs roaring to robots so bright,\n",
      "He's changing the timeline, with all of his might!\n",
      "\n",
      "(Verse 4)\n",
      "He met King Tut, and shared a walnut or two,\n",
      "He helped Marie Curie, with her research to do.\n",
      "He painted with Da Vinci, a masterpiece small,\n",
      "A tiny oak tree, upon a museum wall!\n",
      "Each jump through time, a chaotic delight,\n",
      "Nutsy the squirrel, setting everything right!\n",
      "\n",
      "(Outro)\n",
      "Finally back home, with a tired little sigh,\n",
      "Nutsy the squirrel, under the starry sky.\n",
      "He buried his treasures, a historical stash,\n",
      "A time-traveling squirrel, in a single grand flash!\n",
      "So next time you see one, remember his quest,\n",
      "Nutsy the squirrel, putting history to the test!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = await client.aio.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Compose a song about the adventures of a time-traveling squirrel.\",\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gV1dR-QlTKRs"
   },
   "source": [
    "## Count tokens and compute tokens\n",
    "\n",
    "You can use the `count_tokens` method to calculate the number of input tokens before sending a request to the Gemini API. See the [List and count tokens](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/list-token) page for more details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Syx-fwLkV1j-"
   },
   "source": [
    "#### Count tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "UhNElguLRRNK",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_tokens=9 cached_content_token_count=None\n"
     ]
    }
   ],
   "source": [
    "response = client.models.count_tokens(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What's the highest mountain in Africa?\",\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VS-AP7AHUQmV"
   },
   "source": [
    "#### Compute tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Cdhi5AX1TuH0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens_info=[TokensInfo(role='user', token_ids=[1841, 235303, 235256, 573, 32514, 2204, 575, 573, 4645, 5255, 235336], tokens=[b'What', b\"'\", b's', b' the', b' longest', b' word', b' in', b' the', b' English', b' language', b'?'])]\n"
     ]
    }
   ],
   "source": [
    "response = client.models.compute_tokens(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What's the longest word in the English language?\",\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0pb-Kh1xEHU"
   },
   "source": [
    "## Function calling\n",
    "\n",
    "[Function calling](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling) lets you provide a set of tools a model can use to respond to the user's prompt. You create a description of a function in your code, then pass that description to a language model in a request. The response from the model includes the name of a function that matches the description and the arguments to call it with.\n",
    "\n",
    "For more examples of Function Calling, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/intro_function_calling.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "2BDQPwgcxRN3",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FunctionCall(id=None, args={'destination': 'Paris'}, name='get_destination')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_destination = FunctionDeclaration(\n",
    "    name=\"get_destination\",\n",
    "    description=\"Get the destination that the user wants to go to\",\n",
    "    parameters={\n",
    "        \"type\": \"OBJECT\",\n",
    "        \"properties\": {\n",
    "            \"destination\": {\n",
    "                \"type\": \"STRING\",\n",
    "                \"description\": \"Destination that the user wants to go to\",\n",
    "            },\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "destination_tool = Tool(\n",
    "    function_declarations=[get_destination],\n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"I'd like to travel to Paris.\",\n",
    "    config=GenerateContentConfig(\n",
    "        tools=[destination_tool],\n",
    "        temperature=0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "response.candidates[0].content.parts[0].function_call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EA1Sn-VQE6_J"
   },
   "source": [
    "## Use context caching\n",
    "\n",
    "[Context caching](https://cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-overview) lets you to store frequently used input tokens in a dedicated cache and reference those tokens for subsequent requests. This eliminates the need to repeatedly pass the same set of tokens to a model.\n",
    "\n",
    "**Note**: Context caching is only available for stable models with fixed versions (for example, `gemini-2.0-flash-001`). You must include the version postfix (for example, the `-001`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqxTesUPIkNC"
   },
   "source": [
    "#### Create a cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "adsuvFDA6xP5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_instruction = \"\"\"\n",
    "  You are an expert researcher who has years of experience in conducting systematic literature surveys and meta-analyses of different topics.\n",
    "  You pride yourself on incredible accuracy and attention to detail. You always stick to the facts in the sources provided, and never make up new facts.\n",
    "  Now look at the research paper below, and answer the following questions in 1-2 sentences.\n",
    "\"\"\"\n",
    "\n",
    "pdf_parts = [\n",
    "    Part.from_uri(\n",
    "        file_uri=\"gs://cloud-samples-data/generative-ai/pdf/2312.11805v3.pdf\",\n",
    "        mime_type=\"application/pdf\",\n",
    "    ),\n",
    "    Part.from_uri(\n",
    "        file_uri=\"gs://cloud-samples-data/generative-ai/pdf/2403.05530.pdf\",\n",
    "        mime_type=\"application/pdf\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "cached_content = client.caches.create(\n",
    "    model=\"gemini-2.0-flash-001\",\n",
    "    config=CreateCachedContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "        contents=pdf_parts,\n",
    "        ttl=\"3600s\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBdQNHEoJmC5"
   },
   "source": [
    "#### Use a cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "N8EhgCzlIoFI",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The research paper introduces Gemini, a new family of multimodal models with remarkable capabilities across image, audio, video, and text understanding. The overarching goal is to create models with strong generalist skills across modalities and exceptional understanding and reasoning within each domain.\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash-001\",\n",
    "    contents=\"What is the research goal shared by these research papers?\",\n",
    "    config=GenerateContentConfig(\n",
    "        cached_content=cached_content.name,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azhqrdiCer19"
   },
   "source": [
    "#### Delete a cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "rAUYcfOUdeoi",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeleteCachedContentResponse()"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.caches.delete(name=cached_content.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43be33d2672b"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch prediction\n",
    "\n",
    "While online (synchronous) responses limits you to one input request at a time, [batch predictions for the Gemini API in Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/batch-prediction-gemini) allow you to send a large number of requests to Gemini in a single batch request. Then, the model responses asynchronously populate to your storage output location in [Cloud Storage](https://cloud.google.com/storage/docs/introduction) or [BigQuery](https://cloud.google.com/bigquery/docs/storage_overview).\n",
    "\n",
    "Batch predictions are generally more efficient and cost-effective than online predictions when processing a large number of inputs that are not latency sensitive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adf948ae326b"
   },
   "source": [
    "### Prepare batch inputs\n",
    "\n",
    "The input for batch requests specifies the items to send to your model for prediction.\n",
    "\n",
    "Batch requests for Gemini accept BigQuery storage sources and Cloud Storage sources. Learn more about the batch input formats in the [Batch text generation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/batch-prediction-gemini#prepare_your_inputs) page.\n",
    "\n",
    "This tutorial uses Cloud Storage as an example. The requirements for Cloud Storage input are:\n",
    "\n",
    "- File format: [JSON Lines (JSONL)](https://jsonlines.org/)\n",
    "- Located in `us-central1`\n",
    "- Appropriate read permissions for the service account\n",
    "\n",
    "Each request that you send to a model can include parameters that control how the model generates a response. Learn more about Gemini parameters in the [Experiment with parameter values](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values) page.\n",
    "\n",
    "This is one of the example requests in the input JSONL file `batch_requests_for_multimodal_input_2.jsonl`:\n",
    "\n",
    "```json\n",
    "{\"request\":{\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": \"List objects in this image.\"}, {\"file_data\": {\"file_uri\": \"gs://cloud-samples-data/generative-ai/image/office-desk.jpeg\", \"mime_type\": \"image/jpeg\"}}]}],\"generationConfig\":{\"temperature\": 0.4}}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "81b25154a51a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "INPUT_DATA = \"gs://cloud-samples-data/generative-ai/batch/batch_requests_for_multimodal_input_2.jsonl\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2031bb3f44c2"
   },
   "source": [
    "### Prepare batch output location\n",
    "\n",
    "When a batch prediction task completes, the output is stored in the location specified in your request.\n",
    "\n",
    "- The location is in the form of a Cloud Storage or BigQuery URI prefix, for example:\n",
    "`gs://path/to/output/data` or `bq://projectId.bqDatasetId`.\n",
    "\n",
    "- If not specified, `gs://STAGING_BUCKET/gen-ai-batch-prediction` is used for Cloud Storage source and `bq://PROJECT_ID.gen_ai_batch_prediction.predictions_TIMESTAMP` is used for BigQuery source.\n",
    "\n",
    "This tutorial uses a Cloud Storage bucket as an example for the output location.\n",
    "\n",
    "- You can specify the URI of your Cloud Storage bucket in `BUCKET_URI`, or\n",
    "- if it is not specified, a new Cloud Storage bucket in the form of `gs://PROJECT_ID-TIMESTAMP` is created for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "fddd98cd84cd",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://qwiklabs-gcp-01-7fbe346d670b-20250507131910/...\n"
     ]
    }
   ],
   "source": [
    "BUCKET_URI = \"[your-cloud-storage-bucket]\"  # @param {type:\"string\"}\n",
    "\n",
    "if BUCKET_URI == \"[your-cloud-storage-bucket]\":\n",
    "    TIMESTAMP = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    BUCKET_URI = f\"gs://{PROJECT_ID}-{TIMESTAMP}\"\n",
    "\n",
    "! gsutil mb -l {LOCATION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7da62c98880"
   },
   "source": [
    "### Send a batch prediction request\n",
    "\n",
    "To make a batch prediction request, you specify a source model ID, an input source, and an output location where Vertex AI stores the batch prediction results.\n",
    "\n",
    "To learn more, see the [Batch prediction API](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/batch-prediction-api) page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "7ed3c2925663",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'projects/157060040912/locations/us-west1/batchPredictionJobs/4435551127605346304'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_job = client.batches.create(\n",
    "    model=MODEL_ID,\n",
    "    src=INPUT_DATA,\n",
    "    config=CreateBatchJobConfig(dest=BUCKET_URI),\n",
    ")\n",
    "batch_job.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1bd49ff2c9e"
   },
   "source": [
    "Print out the job status and other properties. You can also check the status in the console at https://console.cloud.google.com/vertex-ai/batch-predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "ee2ec586e4f1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_job = client.batches.get(name=batch_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64eaf082ecb0"
   },
   "source": [
    "Optionally, you can list all the batch prediction jobs in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "da8e9d43a89b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projects/157060040912/locations/us-west1/batchPredictionJobs/4435551127605346304 2025-05-07 13:19:13.813916+00:00 JobState.JOB_STATE_PENDING\n"
     ]
    }
   ],
   "source": [
    "for job in client.batches.list():\n",
    "    print(job.name, job.create_time, job.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de178468ba15"
   },
   "source": [
    "### Wait for the batch prediction job to complete\n",
    "\n",
    "Depending on the number of input items that you submitted, a batch generation task can take some time to complete. Use the following code to check the job status and wait for the job to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "c2187c091738",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job failed: None\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Refresh the job until complete\n",
    "while batch_job.state == \"JOB_STATE_RUNNING\":\n",
    "    time.sleep(5)\n",
    "    batch_job = client.batches.get(name=batch_job.name)\n",
    "\n",
    "# Check if the job succeeds\n",
    "if batch_job.state == \"JOB_STATE_SUCCEEDED\":\n",
    "    print(\"Job succeeded!\")\n",
    "else:\n",
    "    print(f\"Job failed: {batch_job.error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0156eaf66675"
   },
   "source": [
    "### Retrieve batch prediction results\n",
    "\n",
    "When a batch prediction task is complete, the output of the prediction is stored in the location specified in your request. It is also available in `batch_job.dest.bigquery_uri` or `batch_job.dest.gcs_uri`.\n",
    "\n",
    "Example output:\n",
    "\n",
    "```json\n",
    "{\"status\": \"\", \"processed_time\": \"2024-11-13T14:04:28.376+00:00\", \"request\": {\"contents\": [{\"parts\": [{\"file_data\": null, \"text\": \"List objects in this image.\"}, {\"file_data\": {\"file_uri\": \"gs://cloud-samples-data/generative-ai/image/gardening-tools.jpeg\", \"mime_type\": \"image/jpeg\"}, \"text\": null}], \"role\": \"user\"}], \"generationConfig\": {\"temperature\": 0.4}}, \"response\": {\"candidates\": [{\"avgLogprobs\": -0.10394711927934126, \"content\": {\"parts\": [{\"text\": \"Here's a list of the objects in the image:\\n\\n* **Watering can:** A green plastic watering can with a white rose head.\\n* **Plant:** A small plant (possibly oregano) in a terracotta pot.\\n* **Terracotta pots:** Two terracotta pots, one containing the plant and another empty, stacked on top of each other.\\n* **Gardening gloves:** A pair of striped gardening gloves.\\n* **Gardening tools:** A small trowel and a hand cultivator (hoe).  Both are green with black handles.\"}], \"role\": \"model\"}, \"finishReason\": \"STOP\"}], \"modelVersion\": \"gemini-2.0-flash-001@default\", \"usageMetadata\": {\"candidatesTokenCount\": 110, \"promptTokenCount\": 264, \"totalTokenCount\": 374}}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "c2ce0968112c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import fsspec\n",
    "import pandas as pd\n",
    "\n",
    "fs = fsspec.filesystem(\"gcs\")\n",
    "\n",
    "file_paths = fs.glob(f\"{batch_job.dest.gcs_uri}/*/predictions.jsonl\")\n",
    "\n",
    "if batch_job.state == \"JOB_STATE_SUCCEEDED\":\n",
    "    # Load the JSONL file into a DataFrame\n",
    "    df = pd.read_json(f\"gs://{file_paths[0]}\", lines=True)\n",
    "\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f81ccNPjiVzH"
   },
   "source": [
    "## Get text embeddings\n",
    "\n",
    "You can get text embeddings for a snippet of text by using the `embed_content` method. While all models produce an output with 768 dimensions by default, some models give users the option to choose an output dimensionality between `1` and `768`. See [Vertex AI text embeddings API](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "zGOCzT7y31rk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "TEXT_EMBEDDING_MODEL_ID = \"text-embedding-005\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "s94DkG5JewHJ",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ContentEmbedding(values=[-0.07045752555131912, 0.032266948372125626, 0.016506649553775787, -0.03975583240389824, -0.03190124034881592, 0.05326863378286362, 0.04023094102740288, 0.015076016075909138, -0.032303109765052795, 0.006484445184469223, -0.0024321170058101416, 0.011177822947502136, 0.0337241068482399, -0.025560013949871063, -0.018441712483763695, 0.019186269491910934, 0.015283623710274696, -0.03598617762327194, -0.04253664240241051, -0.028812961652874947, 0.0026159759145230055, -0.0029092272743582726, -0.017144568264484406, -0.014058108441531658, -0.06653711199760437, 0.0311944168061018, -0.00982401892542839, 0.011957993730902672, 0.03449905663728714, -0.0309318657964468, 0.030545806512236595, -0.006762423552572727, 0.05796542391180992, 0.06404424458742142, -0.021345125511288643, -0.05859833583235741, 0.015458089299499989, -0.025612354278564453, 0.0018945581978186965, -0.033962976187467575, 0.03931983560323715, -0.06359726935625076, -0.10694780200719833, 0.014972669072449207, 0.036555271595716476, 0.08227287977933884, -0.04199172928929329, 0.01982123777270317, -0.0801253467798233, 0.022590266540646553, 0.039794985204935074, 0.034575168043375015, -0.006564989686012268, -0.02023416757583618, -0.031998686492443085, 0.0423390232026577, -0.02992982417345047, -0.013591731898486614, 0.033661145716905594, 0.07082237303256989, 0.008499796502292156, -0.04865473136305809, -0.002240255009382963, 0.000133246008772403, -0.06205907464027405, -0.09021010249853134, 0.003017326118424535, 0.009957147762179375, 0.036790668964385986, 0.045988310128450394, -0.025111189112067223, -0.022036859765648842, 0.07070532441139221, 0.009957482106983662, -0.005083127412945032, -0.03338221460580826, 0.02343789115548134, 0.01667286455631256, -0.0507337749004364, -0.04134524613618851, -0.05723491683602333, 0.005803215783089399, -0.007348287384957075, -0.04580540210008621, -0.03365001454949379, -0.05468742549419403, -0.004117684438824654, -0.05136139690876007, -0.03516760095953941, -0.047195497900247574, 0.019801415503025055, -0.068564273416996, 0.015214627608656883, 0.02218555472791195, 0.014502515085041523, 0.0535530149936676, 0.07894973456859589, 0.10622807592153549, -0.10025843977928162, 0.003122666385024786, -0.04228673502802849, 0.06404853612184525, -0.0014932050835341215, -0.003603595308959484, -0.02391131781041622, -0.01967342011630535, -0.002356563229113817, -0.020007967948913574, 0.014360041357576847, -0.06779304891824722, -0.016298115253448486, 0.0023577238898724318, -0.005765872076153755, 0.0023203580640256405, -0.013192295096814632, -0.056412022560834885, 0.007363807875663042, -0.05415644124150276, -0.05731481686234474, -0.0473717637360096, -0.010678972117602825, -0.03670541197061539, 0.029316440224647522, 0.07834959030151367, -0.050921354442834854, 0.023325718939304352, 0.01781112514436245, 0.004857619293034077], statistics=ContentEmbeddingStatistics(truncated=False, token_count=18.0)), ContentEmbedding(values=[-0.040600020438432693, 0.01235074084252119, -0.019680218771100044, -0.01208257395774126, -0.023434359580278397, 0.03601039946079254, 0.07041875272989273, 0.016988124698400497, -0.03544733673334122, 0.04181361943483353, 0.0034712895285338163, -0.06320357322692871, -0.011010679416358471, -0.03305432200431824, -0.015262528322637081, 0.009415878914296627, 0.03613364323973656, -0.002960699377581477, -0.004089315887540579, -0.015835143625736237, 0.03513144701719284, -0.01699681580066681, -0.027534503489732742, -0.02328488416969776, -0.0430714376270771, 0.009024102240800858, -0.012769447639584541, 0.011488243006169796, 0.004786378238350153, -0.012272956781089306, 0.014330725185573101, -0.00987081415951252, 0.07926638424396515, 0.06036229431629181, -0.008039862848818302, -0.040001142770051956, 0.02174343727529049, -0.0169086717069149, -0.041982974857091904, -0.011756213381886482, 0.04715228080749512, -0.04051115736365318, -0.058620575815439224, 0.04088404402136803, 0.026120632886886597, 0.054874539375305176, -0.03935108333826065, 0.023588253185153008, -0.08596636354923248, -0.01911880262196064, 0.02538587525486946, 0.05626749247312546, -0.05553530529141426, -0.02841579169034958, -0.06140869855880737, 0.06322570890188217, 0.02559659630060196, 0.02311590500175953, 0.003995103761553764, 0.04910663887858391, 0.010585177689790726, -0.018742388114333153, -0.005701154004782438, 0.023866448551416397, -0.017191538587212563, -0.02421025186777115, -0.014561029151082039, 0.025130953639745712, 0.0678071528673172, 0.03193819150328636, -0.029276639223098755, -0.032992564141750336, 0.031784553080797195, 0.014396755956113338, -0.04835090413689613, -0.05750396102666855, 0.056417543441057205, 0.026612402871251106, 0.003706221003085375, -0.006792127620428801, -0.06691009551286697, 0.03880560025572777, -0.011049091815948486, -0.031246468424797058, -0.030212681740522385, -0.05478524789214134, -0.0013406776124611497, -0.05040643364191055, -0.024219637736678123, -0.05464452505111694, 0.01680794544517994, -0.1249748095870018, 0.059532176703214645, 0.004978098440915346, 0.017065662890672684, 0.046341974288225174, 0.051061924546957016, 0.08931786566972733, -0.0840212032198906, -0.01699705980718136, -0.032077014446258545, 0.015147105790674686, 0.024135185405611992, -0.0014089193427935243, -0.017831386998295784, -0.018690910190343857, -0.056389641016721725, -0.06188696622848511, 0.0756433978676796, -0.01442782673984766, -0.0014380979118868709, 0.0031280883122235537, 0.003938968759030104, -0.0068269590847194195, -0.01963425800204277, -0.05851637199521065, -0.00042248715180903673, -0.05001836270093918, -0.04916193708777428, -0.06605798751115799, 0.008464677259325981, -0.0366244800388813, 0.02593417279422283, 0.05368606746196747, -0.005153220146894455, 0.014385443180799484, 0.050983212888240814, 0.01596856489777565], statistics=ContentEmbeddingStatistics(truncated=False, token_count=10.0)), ContentEmbedding(values=[-0.08152230829000473, 0.013152849860489368, -0.03257665038108826, 0.03197991102933884, -0.04253409057855606, 0.06282426416873932, 0.058349333703517914, -0.021165911108255386, -0.006778216455131769, 0.033225346356630325, 0.010542809031903744, -0.07519274204969406, 0.01183386892080307, -0.018258651718497276, -0.03174315020442009, -0.017948169261217117, 0.04329204186797142, -0.04167334362864494, 0.008950967341661453, 0.015369968488812447, 0.019139297306537628, -0.009914790280163288, -0.017411252483725548, -0.009244228713214397, -0.03578120842576027, -0.013013222254812717, -0.02414533868432045, 0.001035756547935307, -0.016214260831475258, -0.027162395417690277, -0.0054807160049676895, -0.05758097395300865, 0.06951066851615906, 0.02333156019449234, -0.011622974649071693, -0.05659351125359535, 0.03599949926137924, -0.0431552454829216, -0.007645015139132738, 0.0025956102181226015, 0.02833838388323784, -0.08793970197439194, -0.07204460352659225, 0.011749079450964928, 0.045021604746580124, 0.05936109647154808, -0.021462401375174522, -0.0005923219723626971, -0.07067076116800308, -0.0386657677590847, 0.0003290101885795593, 0.0902971550822258, -0.028865663334727287, -0.07067389041185379, -0.06398309022188187, 0.053239382803440094, 0.014394483529031277, 0.028766246512532234, 0.02622423693537712, 0.08315692096948624, 0.012001410126686096, -0.01796274073421955, -0.04735063016414642, 0.04483312740921974, 0.016355058178305626, -0.04842698946595192, 0.005748494993895292, 0.03360263258218765, 0.06504500657320023, 0.010348251089453697, -0.006146855186671019, -0.051634009927511215, 0.043523456901311874, -0.0038610969204455614, -0.03951967507600784, -0.025342371314764023, 0.07996957004070282, 0.02915206179022789, 0.011532247997820377, -0.010988202877342701, -0.03222794085741043, 0.01302323117852211, -0.02658616378903389, -0.013707761652767658, -0.05092770233750343, -0.013486696407198906, -0.04149186611175537, -0.02143094129860401, -0.03536752238869667, -0.0272765401750803, 0.014350875280797482, -0.08868151903152466, 0.0291373822838068, -0.005801372230052948, -0.01348965521901846, 0.021268710494041443, 0.063230000436306, 0.08822762966156006, -0.10215342044830322, -0.013053481467068195, -0.03695044666528702, -0.02952970378100872, 0.015198579989373684, -0.015535973012447357, -0.041377533227205276, -0.014440011233091354, -0.05201679840683937, -0.030554667115211487, 0.07688234746456146, -0.01912546344101429, 0.017124148085713387, -0.02142583578824997, 0.021098323166370392, 0.003955203574150801, -0.05848342180252075, -0.03201206028461456, -0.024658098816871643, -0.05020337179303169, -0.04261450842022896, -0.03640974313020706, 0.01540687121450901, -0.03000785782933235, 0.03421954810619354, 0.019490478560328484, 0.014071287587285042, 0.056027792394161224, 0.051268234848976135, 0.01663997955620289], statistics=ContentEmbeddingStatistics(truncated=False, token_count=13.0))]\n"
     ]
    }
   ],
   "source": [
    "response = client.models.embed_content(\n",
    "    model=TEXT_EMBEDDING_MODEL_ID,\n",
    "    contents=[\n",
    "        \"How do I get a driver's license/learner's permit?\",\n",
    "        \"How do I renew my driver's license?\",\n",
    "        \"How do I change my address on my driver's license?\",\n",
    "    ],\n",
    "    config=EmbedContentConfig(output_dimensionality=128),\n",
    ")\n",
    "\n",
    "print(response.embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQwiONFdVHw5"
   },
   "source": [
    "# What's next\n",
    "\n",
    "- Explore other notebooks in the [Google Cloud Generative AI GitHub repository](https://github.com/GoogleCloudPlatform/generative-ai).\n",
    "- Explore AI models in [Model Garden](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "intro_genai_sdk.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
